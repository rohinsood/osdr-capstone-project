{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Development"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LogisticRegression, Ridge, Lasso\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.svm import SVR\n",
    "import shap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataframe shape: (264, 16678)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array(['Earth', 'SFug', 'SF1g', 'Earth_log', 'SFug_log', 'SF1g_log',\n",
       "       'Earth_sqrt', 'SFug_sqrt', 'SF1g_sqrt', 'Earth_boxcox',\n",
       "       'SFug_boxcox', 'SF1g_boxcox', 'Earth_clr', 'SFug_clr', 'SF1g_clr',\n",
       "       'Earth_deseq2', 'SFug_deseq2', 'SF1g_deseq2', 'Earth_zscore',\n",
       "       'SFug_zscore', 'SF1g_zscore', 'Earth_quantile', 'SFug_quantile',\n",
       "       'SF1g_quantile', 'Earth_minmax', 'SFug_minmax', 'SF1g_minmax',\n",
       "       'Earth_robust', 'SFug_robust', 'SF1g_robust', 'Earth_yeojohnson',\n",
       "       'SFug_yeojohnson', 'SF1g_yeojohnson'], dtype=object)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"data/datasets/augmented_gene_expression_data.csv\")\n",
    "results_df = pd.DataFrame(columns=['Model', 'Target Variable', 'MSE', 'RMSE', 'MAE', 'R2', 'R', \"Selected Features\", \"Top 20 SHAP Values\"])\n",
    "df.drop(columns=['Unnamed: 0'], inplace=True)\n",
    "target_columns = df.columns[-2:].to_list()\n",
    "\n",
    "print(\"Dataframe shape:\", df.shape)\n",
    "\n",
    "# Extract environments from source_name\n",
    "df['Environment'] = df['source_name'].apply(lambda x: f\"{x.split('_')[0]}_{x.split('_')[-1]}\" if x.count('_') == 2 else x.split('_')[0])\n",
    "environments = df['Environment'].unique()\n",
    "environments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Earth', 'SF1g', 'SFug'], dtype='<U5')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_environments = np.array([x.split('_')[0] for x in environments])\n",
    "raw_environments = np.unique(raw_environments)\n",
    "raw_environments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([], dtype=float64)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "synth_environments = np.array([env for env in environments if env not in environments])\n",
    "synth_environments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['th_positive_cells', 'repo_glial_cells']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>th_positive_cells</th>\n",
       "      <th>repo_glial_cells</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>264.000000</td>\n",
       "      <td>264.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>24.391343</td>\n",
       "      <td>93.868016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>39.085647</td>\n",
       "      <td>192.667465</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>-5.199338</td>\n",
       "      <td>-5.199338</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.082538</td>\n",
       "      <td>-0.007168</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>1.141754</td>\n",
       "      <td>1.062169</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>45.651284</td>\n",
       "      <td>27.588908</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>120.545144</td>\n",
       "      <td>810.810964</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       th_positive_cells  repo_glial_cells\n",
       "count         264.000000        264.000000\n",
       "mean           24.391343         93.868016\n",
       "std            39.085647        192.667465\n",
       "min            -5.199338         -5.199338\n",
       "25%             0.082538         -0.007168\n",
       "50%             1.141754          1.062169\n",
       "75%            45.651284         27.588908\n",
       "max           120.545144        810.810964"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_variables = df[target_columns]\n",
    "target_variables.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['th_positive_cells', 'repo_glial_cells', 'Environment', 'source_name']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "drop_columns = target_columns.copy()\n",
    "drop_columns.extend(['Environment', 'source_name'])\n",
    "drop_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ann_regr(data, target_column, n_features=50):\n",
    "    \n",
    "    # Split the data\n",
    "    train_data, test_data = train_test_split(data, test_size=0.2, random_state=42, stratify=data[\"Environment\"])\n",
    "    val_data, train_data = train_test_split(train_data, train_size=0.1625, random_state=42, stratify=train_data['Environment'])\n",
    "\n",
    "    # Feature selection using RFE on validation data\n",
    "    X_val = val_data.drop(columns=drop_columns)\n",
    "    y_val = val_data[target_column]\n",
    "    \n",
    "    selector = RFE(estimator=DecisionTreeRegressor(), n_features_to_select=n_features, step=10)\n",
    "    selector = selector.fit(X_val, y_val)\n",
    "    \n",
    "    # Select the important features\n",
    "    X_val_selected = selector.transform(X_val)\n",
    "    \n",
    "    # Scale the features\n",
    "    scaler = StandardScaler()\n",
    "    X_val_scaled = scaler.fit_transform(X_val_selected)\n",
    "    \n",
    "    # Define the parameter grid for GridSearchCV\n",
    "    param_grid = {\n",
    "        'hidden_layer_sizes': [(64, 32, 16), (128, 64, 32), (32, 16, 8)],\n",
    "        'activation': ['relu', 'tanh'],\n",
    "        'solver': ['adam', 'sgd'],\n",
    "        'alpha': [0.0001, 0.001, 0.01],\n",
    "        'learning_rate': ['constant', 'adaptive'],\n",
    "        'max_iter': [800, 1200, 1600]\n",
    "    }\n",
    "    \n",
    "    # Initialize the MLP model\n",
    "    mlp = MLPRegressor(random_state=42)\n",
    "    \n",
    "    # Perform grid search with cross-validation\n",
    "    grid_search = GridSearchCV(estimator=mlp, param_grid=param_grid, cv=3, scoring='neg_mean_squared_error', n_jobs=-1, verbose=1)\n",
    "    grid_search.fit(X_val_scaled, y_val)\n",
    "    \n",
    "    # Get the best model\n",
    "    best_mlp = grid_search.best_estimator_\n",
    "    \n",
    "    # k-fold cross validation, trains in one, tests in another. eliminates biases\n",
    "    for e_train in environments:\n",
    "        for e_test in environments:\n",
    "            \n",
    "            if e_train == e_test:\n",
    "                continue\n",
    "            \n",
    "            print(\"Training in raw environment:\", e_train)\n",
    "            print(\"Testing in transformed environment:\", e_test)\n",
    "\n",
    "            train_data_env = train_data[train_data['Environment'] == e_train]\n",
    "            test_data_env = train_data[train_data['Environment'] == e_test]\n",
    "            \n",
    "            if train_data_env.empty or test_data_env.empty:\n",
    "                continue\n",
    "            \n",
    "            X_train = train_data_env.drop(columns=drop_columns)\n",
    "            y_train = train_data_env[target_column]\n",
    "\n",
    "            X_test = test_data_env.drop(columns=drop_columns)\n",
    "            y_test = test_data_env[target_column]\n",
    "\n",
    "            X_train_selected = selector.transform(X_train)\n",
    "            X_train_scaled = scaler.transform(X_train_selected)\n",
    "\n",
    "            X_test_selected = selector.transform(X_test)\n",
    "            X_test_scaled = scaler.transform(X_test_selected)\n",
    "\n",
    "            # Train the model with the best parameters\n",
    "            best_mlp.fit(X_train_scaled, y_train)\n",
    "\n",
    "    # Final evaluation on the 20% test set\n",
    "    X_test = test_data.drop(columns=drop_columns)\n",
    "    y_test = test_data[target_column]\n",
    "\n",
    "    X_test_selected = selector.transform(X_test)\n",
    "    X_test_scaled = scaler.transform(X_test_selected)\n",
    "\n",
    "    y_pred = best_mlp.predict(X_test_scaled)\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    rmse = np.sqrt(mse)\n",
    "    mae = mean_absolute_error(y_test, y_pred)\n",
    "    r2 = r2_score(y_test, y_pred)\n",
    "    r = np.sqrt(r2)\n",
    "    \n",
    "    selected_features = X_test.columns[selector.support_]\n",
    "\n",
    "    # Compute SHAP values to get feature importance\n",
    "    explainer = shap.KernelExplainer(best_mlp.predict, X_test_scaled)\n",
    "    shap_values = explainer.shap_values(X_test_scaled)\n",
    "    shap_importance = np.abs(shap_values).mean(axis=0)\n",
    "\n",
    "    # Combine feature names with SHAP importance values\n",
    "    shap_feature_importance = dict(zip(selected_features, shap_importance))\n",
    "\n",
    "    # Sort features by SHAP importance and select the top 20\n",
    "    top_20_features = sorted(shap_feature_importance.items(), key=lambda x: x[1], reverse=True)[:20]\n",
    "    top_20_features_dict = dict(top_20_features)\n",
    "\n",
    "    # Store the final test results\n",
    "    results_df.loc[len(results_df)] = [\n",
    "        \"ANN_Final_Test\", target_column, mse, rmse, mae, r2, r, ','.join(selected_features),\n",
    "        str(top_20_features_dict)              # Store top 20 SHAP values\n",
    "    ]\n",
    "\n",
    "    return results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lr_regr(data, target_column, n_features=50):\n",
    "    \n",
    "    train_data, test_data = train_test_split(data, test_size=0.2, random_state=42, stratify=data[\"Environment\"])\n",
    "    \n",
    "    val_data, train_data = train_test_split(train_data, train_size=0.1625, random_state=42, stratify=train_data['Environment'])\n",
    "\n",
    "    # Feature selection using RFE on validation data\n",
    "    X_val = val_data.drop(columns=drop_columns)\n",
    "    y_val = val_data[target_column]\n",
    "    \n",
    "    selector = RFE(estimator=LinearRegression(), n_features_to_select=n_features, step=10)\n",
    "    selector = selector.fit(X_val, y_val)\n",
    "    \n",
    "    # Select the important features\n",
    "    X_val_selected = selector.transform(X_val)\n",
    "    \n",
    "    # Scale the features\n",
    "    scaler = StandardScaler()\n",
    "    X_val_scaled = scaler.fit_transform(X_val_selected)\n",
    "    \n",
    "    # Initialize the Linear Regression model\n",
    "    lr = LinearRegression()\n",
    "    \n",
    "    # No need for GridSearchCV, as LinearRegression has no hyperparameters to tune\n",
    "    best_lr = lr\n",
    "    \n",
    "    # Training in raw environment and testing in transformed environment\n",
    "    for e_train in environments:\n",
    "        for e_test in environments:\n",
    "            \n",
    "            if e_train == e_test:\n",
    "                continue\n",
    "            \n",
    "            print(\"Training in raw environment:\", e_train)\n",
    "            print(\"Testing in transformed environment:\", e_test)\n",
    "\n",
    "            train_data_env = train_data[train_data['Environment'] == e_train]\n",
    "            test_data_env = train_data[train_data['Environment'] == e_test]\n",
    "            \n",
    "            if train_data_env.empty or test_data_env.empty:\n",
    "                continue\n",
    "            \n",
    "            X_train = train_data_env.drop(columns=drop_columns)\n",
    "            y_train = train_data_env[target_column]\n",
    "\n",
    "            X_test = test_data_env.drop(columns=drop_columns)\n",
    "            y_test = test_data_env[target_column]\n",
    "\n",
    "            X_train_selected = selector.transform(X_train)\n",
    "            X_train_scaled = scaler.transform(X_train_selected)\n",
    "\n",
    "            X_test_selected = selector.transform(X_test)\n",
    "            X_test_scaled = scaler.transform(X_test_selected)\n",
    "\n",
    "            # Train the model with the selected features\n",
    "            best_lr.fit(X_train_scaled, y_train)\n",
    "\n",
    "    # Final evaluation on the 20% test set\n",
    "    X_test = test_data.drop(columns=drop_columns)\n",
    "    y_test = test_data[target_column]\n",
    "\n",
    "    X_test_selected = selector.transform(X_test)\n",
    "    X_test_scaled = scaler.transform(X_test_selected)\n",
    "\n",
    "    y_pred = best_lr.predict(X_test_scaled)\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    rmse = np.sqrt(mse)\n",
    "    mae = mean_absolute_error(y_test, y_pred)\n",
    "    r2 = r2_score(y_test, y_pred)\n",
    "    r = np.sqrt(r2)\n",
    "    \n",
    "    selected_features = X_test.columns[selector.support_]\n",
    "\n",
    "    # Extracting the coefficients (weights) of the linear regression model\n",
    "    coefficients = best_lr.coef_\n",
    "    coefficients_str = ','.join(map(str, coefficients))\n",
    "    \n",
    "    # Compute SHAP values to get feature importance\n",
    "    explainer = shap.KernelExplainer(best_lr.predict, X_test_scaled)\n",
    "    shap_values = explainer.shap_values(X_test_scaled)\n",
    "    shap_importance = np.abs(shap_values).mean(axis=0)\n",
    "\n",
    "    # Combine feature names with SHAP importance values\n",
    "    shap_feature_importance = dict(zip(selected_features, shap_importance))\n",
    "\n",
    "    # Sort features by SHAP importance and select the top 20\n",
    "    top_20_features = sorted(shap_feature_importance.items(), key=lambda x: x[1], reverse=True)[:20]\n",
    "    top_20_features_dict = dict(top_20_features)\n",
    "\n",
    "    # Store the final test results\n",
    "    results_df.loc[len(results_df)] = [\n",
    "        \"Linear_Regression_Final_Test\", target_column, mse, rmse, mae, r2, r, ','.join(selected_features),\n",
    "        str(top_20_features_dict)    # Store top 20 SHAP values\n",
    "    ]\n",
    "\n",
    "    return results_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rf_regr(data, target_column, n_features=50):\n",
    "    \n",
    "    train_data, test_data = train_test_split(data, test_size=0.2, random_state=42, stratify=data[\"Environment\"])\n",
    "    \n",
    "    val_data, train_data = train_test_split(train_data, train_size=0.1625, random_state=42, stratify=train_data['Environment'])\n",
    "\n",
    "    # Feature selection using RFE on validation data\n",
    "    X_val = val_data.drop(columns=drop_columns)\n",
    "    y_val = val_data[target_column]\n",
    "    \n",
    "    selector = RFE(estimator=RandomForestRegressor(), n_features_to_select=n_features, step=10)\n",
    "    selector = selector.fit(X_val, y_val)\n",
    "    \n",
    "    # Select the important features\n",
    "    X_val_selected = selector.transform(X_val)\n",
    "    \n",
    "    # Scale the features\n",
    "    scaler = StandardScaler()\n",
    "    X_val_scaled = scaler.fit_transform(X_val_selected)\n",
    "    \n",
    "    # Define the parameter grid for GridSearchCV\n",
    "    param_grid = {\n",
    "        'n_estimators': [100, 200, 300],\n",
    "        'max_depth': [10, 20, None],\n",
    "        'min_samples_split': [2, 5, 10],\n",
    "        'min_samples_leaf': [1, 2, 4],\n",
    "        'bootstrap': [True, False]\n",
    "    }\n",
    "    \n",
    "    # Initialize the Random Forest model\n",
    "    rf = RandomForestRegressor(random_state=42)\n",
    "    \n",
    "    # Perform grid search with cross-validation\n",
    "    grid_search = GridSearchCV(estimator=rf, param_grid=param_grid, cv=3, scoring='neg_mean_squared_error', n_jobs=-1, verbose=1)\n",
    "    grid_search.fit(X_val_scaled, y_val)\n",
    "    \n",
    "    # Get the best model\n",
    "    best_rf = grid_search.best_estimator_\n",
    "    \n",
    "    # Training in raw environment and testing in transformed environment\n",
    "    for e_train in environments:\n",
    "        for e_test in environments:\n",
    "            \n",
    "            if e_train == e_test:\n",
    "                continue\n",
    "            \n",
    "            print(\"Training in raw environment:\", e_train)\n",
    "            print(\"Testing in transformed environment:\", e_test)\n",
    "\n",
    "            train_data_env = train_data[train_data['Environment'] == e_train]\n",
    "            test_data_env = train_data[train_data['Environment'] == e_test]\n",
    "            \n",
    "            if train_data_env.empty or test_data_env.empty:\n",
    "                continue\n",
    "            \n",
    "            X_train = train_data_env.drop(columns=drop_columns)\n",
    "            y_train = train_data_env[target_column]\n",
    "\n",
    "            X_test = test_data_env.drop(columns=drop_columns)\n",
    "            y_test = test_data_env[target_column]\n",
    "\n",
    "            X_train_selected = selector.transform(X_train)\n",
    "            X_train_scaled = scaler.transform(X_train_selected)\n",
    "\n",
    "            X_test_selected = selector.transform(X_test)\n",
    "            X_test_scaled = scaler.transform(X_test_selected)\n",
    "\n",
    "            # Train the model with the best parameters\n",
    "            best_rf.fit(X_train_scaled, y_train)\n",
    "\n",
    "    # Final evaluation on the 20% test set\n",
    "    X_test = test_data.drop(columns=drop_columns)\n",
    "    y_test = test_data[target_column]\n",
    "\n",
    "    X_test_selected = selector.transform(X_test)\n",
    "    X_test_scaled = scaler.transform(X_test_selected)\n",
    "\n",
    "    y_pred = best_rf.predict(X_test_scaled)\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    rmse = np.sqrt(mse)\n",
    "    mae = mean_absolute_error(y_test, y_pred)\n",
    "    r2 = r2_score(y_test, y_pred)\n",
    "    r = np.sqrt(r2)\n",
    "    \n",
    "    selected_features = X_test.columns[selector.support_]\n",
    "\n",
    "    # Compute SHAP values to get feature importance\n",
    "    explainer = shap.TreeExplainer(best_rf)\n",
    "    shap_values = explainer.shap_values(X_test_scaled)\n",
    "    shap_importance = np.abs(shap_values).mean(axis=0)\n",
    "\n",
    "    # Combine feature names with SHAP importance values\n",
    "    shap_feature_importance = dict(zip(selected_features, shap_importance))\n",
    "\n",
    "    # Sort features by SHAP importance and select the top 20\n",
    "    top_20_features = sorted(shap_feature_importance.items(), key=lambda x: x[1], reverse=True)[:20]\n",
    "    top_20_features_dict = dict(top_20_features)\n",
    "\n",
    "    # Store the final test results\n",
    "    results_df.loc[len(results_df)] = [\n",
    "        \"Random_Forest_Final_Test\", target_column, mse, rmse, mae, r2, r, ','.join(selected_features),\n",
    "        str(top_20_features_dict)  # Store top 20 SHAP values\n",
    "    ]\n",
    "\n",
    "    return results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ridge_regr(data, target_column, n_features=50):\n",
    "    \n",
    "    train_data, test_data = train_test_split(data, test_size=0.2, random_state=42, stratify=data[\"Environment\"])\n",
    "    \n",
    "    val_data, train_data = train_test_split(train_data, train_size=0.1625, random_state=42, stratify=train_data['Environment'])\n",
    "\n",
    "    # Feature selection using RFE on validation data\n",
    "    X_val = val_data.drop(columns=drop_columns)\n",
    "    y_val = val_data[target_column]\n",
    "    \n",
    "    selector = RFE(estimator=Ridge(), n_features_to_select=n_features, step=10)\n",
    "    selector = selector.fit(X_val, y_val)\n",
    "    \n",
    "    # Select the important features\n",
    "    X_val_selected = selector.transform(X_val)\n",
    "    \n",
    "    # Scale the features\n",
    "    scaler = StandardScaler()\n",
    "    X_val_scaled = scaler.fit_transform(X_val_selected)\n",
    "    \n",
    "    # Define the parameter grid for GridSearchCV\n",
    "    param_grid = {\n",
    "        'alpha': [0.01, 0.1, 1.0, 10.0],  # Regularization strength\n",
    "        'max_iter': [1000, 5000, 10000]\n",
    "    }\n",
    "    \n",
    "    # Initialize the Ridge model\n",
    "    ridge = Ridge(random_state=42)\n",
    "    \n",
    "    # Perform grid search with cross-validation\n",
    "    grid_search = GridSearchCV(estimator=ridge, param_grid=param_grid, cv=3, scoring='neg_mean_squared_error', n_jobs=-1, verbose=1)\n",
    "    grid_search.fit(X_val_scaled, y_val)\n",
    "    \n",
    "    # Get the best model\n",
    "    best_ridge = grid_search.best_estimator_\n",
    "    \n",
    "    # Training in raw environment and testing in transformed environment\n",
    "    for e_train in environments:\n",
    "        for e_test in environments:\n",
    "            \n",
    "            if e_train == e_test:\n",
    "                continue\n",
    "            \n",
    "            print(\"Training in raw environment:\", e_train)\n",
    "            print(\"Testing in transformed environment:\", e_test)\n",
    "\n",
    "            train_data_env = train_data[train_data['Environment'] == e_train]\n",
    "            test_data_env = train_data[train_data['Environment'] == e_test]\n",
    "            \n",
    "            if train_data_env.empty or test_data_env.empty:\n",
    "                continue\n",
    "            \n",
    "            X_train = train_data_env.drop(columns=drop_columns)\n",
    "            y_train = train_data_env[target_column]\n",
    "\n",
    "            X_test = test_data_env.drop(columns=drop_columns)\n",
    "            y_test = test_data_env[target_column]\n",
    "\n",
    "            X_train_selected = selector.transform(X_train)\n",
    "            X_train_scaled = scaler.transform(X_train_selected)\n",
    "\n",
    "            X_test_selected = selector.transform(X_test)\n",
    "            X_test_scaled = scaler.transform(X_test_selected)\n",
    "\n",
    "            # Train the model with the best parameters\n",
    "            best_ridge.fit(X_train_scaled, y_train)\n",
    "\n",
    "    # Final evaluation on the 20% test set\n",
    "    X_test = test_data.drop(columns=drop_columns)\n",
    "    y_test = test_data[target_column]\n",
    "\n",
    "    X_test_selected = selector.transform(X_test)\n",
    "    X_test_scaled = scaler.transform(X_test_selected)\n",
    "\n",
    "    y_pred = best_ridge.predict(X_test_scaled)\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    rmse = np.sqrt(mse)\n",
    "    mae = mean_absolute_error(y_test, y_pred)\n",
    "    r2 = r2_score(y_test, y_pred)\n",
    "    r = np.sqrt(r2)\n",
    "    \n",
    "    selected_features = X_test.columns[selector.support_]\n",
    "\n",
    "    # Compute SHAP values to get feature importance\n",
    "    explainer = shap.KernelExplainer(best_ridge.predict, X_test_scaled)\n",
    "    shap_values = explainer.shap_values(X_test_scaled)\n",
    "    shap_importance = np.abs(shap_values).mean(axis=0)\n",
    "\n",
    "    # Combine feature names with SHAP importance values\n",
    "    shap_feature_importance = dict(zip(selected_features, shap_importance))\n",
    "\n",
    "    # Sort features by SHAP importance and select the top 20\n",
    "    top_20_features = sorted(shap_feature_importance.items(), key=lambda x: x[1], reverse=True)[:20]\n",
    "    top_20_features_dict = dict(top_20_features)\n",
    "\n",
    "    # Extracting the coefficients (weights) of the Ridge model\n",
    "    coefficients = best_ridge.coef_\n",
    "    coefficients_str = ','.join(map(str, coefficients))\n",
    "    \n",
    "    # Store the final test results\n",
    "    results_df.loc[len(results_df)] = [\n",
    "        \"Ridge_Final_Test\", target_column, mse, rmse, mae, r2, r, \n",
    "        ','.join(selected_features), str(top_20_features_dict)  # Store top 20 SHAP values\n",
    "    ]\n",
    "\n",
    "    return results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lasso_regr(data, target_column, n_features=50):\n",
    "    \n",
    "    train_data, test_data = train_test_split(data, test_size=0.2, random_state=42, stratify=data[\"Environment\"])\n",
    "    \n",
    "    val_data, train_data = train_test_split(train_data, train_size=0.1625, random_state=42, stratify=train_data['Environment'])\n",
    "\n",
    "    # Feature selection using RFE on validation data\n",
    "    X_val = val_data.drop(columns=drop_columns)\n",
    "    y_val = val_data[target_column]\n",
    "    \n",
    "    selector = RFE(estimator=Lasso(), n_features_to_select=n_features, step=10)\n",
    "    selector = selector.fit(X_val, y_val)\n",
    "    \n",
    "    # Select the important features\n",
    "    X_val_selected = selector.transform(X_val)\n",
    "    \n",
    "    # Scale the features\n",
    "    scaler = StandardScaler()\n",
    "    X_val_scaled = scaler.fit_transform(X_val_selected)\n",
    "    \n",
    "    # Define the parameter grid for GridSearchCV\n",
    "    param_grid = {\n",
    "        'alpha': [0.01, 0.1, 1.0, 10.0],  # Regularization strength\n",
    "        'max_iter': [1000, 5000, 10000]\n",
    "    }\n",
    "    \n",
    "    # Initialize the Lasso model\n",
    "    lasso = Lasso(random_state=42)\n",
    "    \n",
    "    # Perform grid search with cross-validation\n",
    "    grid_search = GridSearchCV(estimator=lasso, param_grid=param_grid, cv=3, scoring='neg_mean_squared_error', n_jobs=-1, verbose=1)\n",
    "    grid_search.fit(X_val_scaled, y_val)\n",
    "    \n",
    "    # Get the best model\n",
    "    best_lasso = grid_search.best_estimator_\n",
    "    \n",
    "    # Training in raw environment and testing in transformed environment\n",
    "    for e_train in environments:\n",
    "        for e_test in environments:\n",
    "            \n",
    "            if e_train == e_test:\n",
    "                continue\n",
    "            \n",
    "            print(\"Training in raw environment:\", e_train)\n",
    "            print(\"Testing in transformed environment:\", e_test)\n",
    "\n",
    "            train_data_env = train_data[train_data['Environment'] == e_train]\n",
    "            test_data_env = train_data[train_data['Environment'] == e_test]\n",
    "            \n",
    "            if train_data_env.empty or test_data_env.empty:\n",
    "                continue\n",
    "            \n",
    "            X_train = train_data_env.drop(columns=drop_columns)\n",
    "            y_train = train_data_env[target_column]\n",
    "\n",
    "            X_test = test_data_env.drop(columns=drop_columns)\n",
    "            y_test = test_data_env[target_column]\n",
    "\n",
    "            X_train_selected = selector.transform(X_train)\n",
    "            X_train_scaled = scaler.transform(X_train_selected)\n",
    "\n",
    "            X_test_selected = selector.transform(X_test)\n",
    "            X_test_scaled = scaler.transform(X_test_selected)\n",
    "\n",
    "            # Train the model with the best parameters\n",
    "            best_lasso.fit(X_train_scaled, y_train)\n",
    "\n",
    "    # Final evaluation on the 20% test set\n",
    "    X_test = test_data.drop(columns=drop_columns)\n",
    "    y_test = test_data[target_column]\n",
    "\n",
    "    X_test_selected = selector.transform(X_test)\n",
    "    X_test_scaled = scaler.transform(X_test_selected)\n",
    "\n",
    "    y_pred = best_lasso.predict(X_test_scaled)\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    rmse = np.sqrt(mse)\n",
    "    mae = mean_absolute_error(y_test, y_pred)\n",
    "    r2 = r2_score(y_test, y_pred)\n",
    "    r = np.sqrt(r2)\n",
    "    \n",
    "    selected_features = X_test.columns[selector.support_]\n",
    "\n",
    "    # Compute SHAP values to get feature importance\n",
    "    explainer = shap.KernelExplainer(best_lasso.predict, X_test_scaled)\n",
    "    shap_values = explainer.shap_values(X_test_scaled)\n",
    "    shap_importance = np.abs(shap_values).mean(axis=0)\n",
    "\n",
    "    # Combine feature names with SHAP importance values\n",
    "    shap_feature_importance = dict(zip(selected_features, shap_importance))\n",
    "\n",
    "    # Sort features by SHAP importance and select the top 20\n",
    "    top_20_features = sorted(shap_feature_importance.items(), key=lambda x: x[1], reverse=True)[:20]\n",
    "    top_20_features_dict = dict(top_20_features)\n",
    "\n",
    "    # Extracting the coefficients (weights) of the Lasso model\n",
    "    coefficients = best_lasso.coef_\n",
    "    coefficients_str = ','.join(map(str, coefficients))\n",
    "    \n",
    "    # Store the final test results\n",
    "    results_df.loc[len(results_df)] = [\n",
    "        \"Lasso_Final_Test\", target_column, mse, rmse, mae, r2, r, \n",
    "        ','.join(selected_features), str(top_20_features_dict)  # Store top 20 SHAP values\n",
    "    ]\n",
    "\n",
    "    return results_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def svm_regr(data, target_column, n_features=50):\n",
    "    \n",
    "    train_data, test_data = train_test_split(data, test_size=0.2, random_state=42, stratify=data[\"Environment\"])\n",
    "    \n",
    "    val_data, train_data = train_test_split(train_data, train_size=0.1625, random_state=42, stratify=train_data['Environment'])\n",
    "\n",
    "    # Feature selection using RFE on validation data\n",
    "    X_val = val_data.drop(columns=drop_columns)\n",
    "    y_val = val_data[target_column]\n",
    "    \n",
    "    selector = RFE(estimator=SVR(kernel='linear'), n_features_to_select=n_features, step=10)\n",
    "    selector = selector.fit(X_val, y_val)\n",
    "    \n",
    "    # Select the important features\n",
    "    X_val_selected = selector.transform(X_val)\n",
    "    \n",
    "    # Scale the features\n",
    "    scaler = StandardScaler()\n",
    "    X_val_scaled = scaler.fit_transform(X_val_selected)\n",
    "    \n",
    "    # Define the parameter grid for GridSearchCV\n",
    "    param_grid = {\n",
    "        'C': [0.01, 0.1, 1.0, 10.0],  # Regularization strength (soft margin parameter)\n",
    "        'epsilon': [0.001, 0.01, 0.1],  # Insensitivity parameter\n",
    "    }\n",
    "    \n",
    "    # Initialize the SVR model\n",
    "    svm = SVR(kernel='linear')\n",
    "    \n",
    "    # Perform grid search with cross-validation\n",
    "    grid_search = GridSearchCV(estimator=svm, param_grid=param_grid, cv=3, scoring='neg_mean_squared_error', n_jobs=-1, verbose=1)\n",
    "    grid_search.fit(X_val_scaled, y_val)\n",
    "    \n",
    "    # Get the best model\n",
    "    best_svm = grid_search.best_estimator_\n",
    "    \n",
    "    # Training in raw environment and testing in transformed environment\n",
    "    for e_train in environments:\n",
    "        for e_test in environments:\n",
    "            \n",
    "            if e_train == e_test:\n",
    "                continue\n",
    "            \n",
    "            print(\"Training in raw environment:\", e_train)\n",
    "            print(\"Testing in transformed environment:\", e_test)\n",
    "\n",
    "            train_data_env = train_data[train_data['Environment'] == e_train]\n",
    "            test_data_env = train_data[train_data['Environment'] == e_test]\n",
    "            \n",
    "            if train_data_env.empty or test_data_env.empty:\n",
    "                continue\n",
    "            \n",
    "            X_train = train_data_env.drop(columns=drop_columns)\n",
    "            y_train = train_data_env[target_column]\n",
    "\n",
    "            X_test = test_data_env.drop(columns=drop_columns)\n",
    "            y_test = test_data_env[target_column]\n",
    "\n",
    "            X_train_selected = selector.transform(X_train)\n",
    "            X_train_scaled = scaler.transform(X_train_selected)\n",
    "\n",
    "            X_test_selected = selector.transform(X_test)\n",
    "            X_test_scaled = scaler.transform(X_test_selected)\n",
    "\n",
    "            # Train the model with the best parameters\n",
    "            best_svm.fit(X_train_scaled, y_train)\n",
    "\n",
    "    # Final evaluation on the 20% test set\n",
    "    X_test = test_data.drop(columns=drop_columns)\n",
    "    y_test = test_data[target_column]\n",
    "\n",
    "    X_test_selected = selector.transform(X_test)\n",
    "    X_test_scaled = scaler.transform(X_test_selected)\n",
    "\n",
    "    y_pred = best_svm.predict(X_test_scaled)\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    rmse = np.sqrt(mse)\n",
    "    mae = mean_absolute_error(y_test, y_pred)\n",
    "    r2 = r2_score(y_test, y_pred)\n",
    "    r = np.sqrt(r2)\n",
    "    \n",
    "    selected_features = X_test.columns[selector.support_]\n",
    "\n",
    "    # Compute SHAP values to get feature importance\n",
    "    explainer = shap.KernelExplainer(best_svm.predict, X_test_scaled)\n",
    "    shap_values = explainer.shap_values(X_test_scaled)\n",
    "    shap_importance = np.abs(shap_values).mean(axis=0)\n",
    "\n",
    "    # Combine feature names with SHAP importance values\n",
    "    shap_feature_importance = dict(zip(selected_features, shap_importance))\n",
    "\n",
    "    # Sort features by SHAP importance and select the top 20\n",
    "    top_20_features = sorted(shap_feature_importance.items(), key=lambda x: x[1], reverse=True)[:20]\n",
    "    top_20_features_dict = dict(top_20_features)\n",
    "\n",
    "    # Extracting the coefficients (weights) of the SVM model\n",
    "    coefficients = best_svm.coef_\n",
    "    coefficients_str = ','.join(map(str, coefficients[0]))  # Flattening the coefficients array\n",
    "    \n",
    "    # Store the final test results\n",
    "    results_df.loc[len(results_df)] = [\n",
    "        \"SVM_Final_Test\", target_column, mse, rmse, mae, r2, r, \n",
    "        ','.join(selected_features), str(top_20_features_dict)  # Store top 20 SHAP values\n",
    "    ]\n",
    "\n",
    "    return results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training MLP for th_positive_cells...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 7\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[39mfor\u001b[39;00m target \u001b[39min\u001b[39;00m target_columns:\n\u001b[1;32m      6\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mTraining MLP for \u001b[39m\u001b[39m{\u001b[39;00mtarget\u001b[39m}\u001b[39;00m\u001b[39m...\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m----> 7\u001b[0m     ann_regr(df, target, n_features\u001b[39m=\u001b[39;49m\u001b[39m1500\u001b[39;49m)\n\u001b[1;32m      8\u001b[0m     results_df\u001b[39m.\u001b[39mto_csv(results_csv_path, index\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n\u001b[1;32m     10\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mTraining Linear Regression for \u001b[39m\u001b[39m{\u001b[39;00mtarget\u001b[39m}\u001b[39;00m\u001b[39m...\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[8], line 12\u001b[0m, in \u001b[0;36mann_regr\u001b[0;34m(data, target_column, n_features)\u001b[0m\n\u001b[1;32m      9\u001b[0m y_val \u001b[39m=\u001b[39m val_data[target_column]\n\u001b[1;32m     11\u001b[0m selector \u001b[39m=\u001b[39m RFE(estimator\u001b[39m=\u001b[39mDecisionTreeRegressor(), n_features_to_select\u001b[39m=\u001b[39mn_features, step\u001b[39m=\u001b[39m\u001b[39m10\u001b[39m)\n\u001b[0;32m---> 12\u001b[0m selector \u001b[39m=\u001b[39m selector\u001b[39m.\u001b[39;49mfit(X_val, y_val)\n\u001b[1;32m     14\u001b[0m \u001b[39m# Select the important features\u001b[39;00m\n\u001b[1;32m     15\u001b[0m X_val_selected \u001b[39m=\u001b[39m selector\u001b[39m.\u001b[39mtransform(X_val)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/sklearn/base.py:1474\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1467\u001b[0m     estimator\u001b[39m.\u001b[39m_validate_params()\n\u001b[1;32m   1469\u001b[0m \u001b[39mwith\u001b[39;00m config_context(\n\u001b[1;32m   1470\u001b[0m     skip_parameter_validation\u001b[39m=\u001b[39m(\n\u001b[1;32m   1471\u001b[0m         prefer_skip_nested_validation \u001b[39mor\u001b[39;00m global_skip_validation\n\u001b[1;32m   1472\u001b[0m     )\n\u001b[1;32m   1473\u001b[0m ):\n\u001b[0;32m-> 1474\u001b[0m     \u001b[39mreturn\u001b[39;00m fit_method(estimator, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/sklearn/feature_selection/_rfe.py:264\u001b[0m, in \u001b[0;36mRFE.fit\u001b[0;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[1;32m    244\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Fit the RFE model and then the underlying estimator on the selected features.\u001b[39;00m\n\u001b[1;32m    245\u001b[0m \n\u001b[1;32m    246\u001b[0m \u001b[39mParameters\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    261\u001b[0m \u001b[39m    Fitted estimator.\u001b[39;00m\n\u001b[1;32m    262\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    263\u001b[0m _raise_for_unsupported_routing(\u001b[39mself\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mfit\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mfit_params)\n\u001b[0;32m--> 264\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_fit(X, y, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mfit_params)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/sklearn/feature_selection/_rfe.py:311\u001b[0m, in \u001b[0;36mRFE._fit\u001b[0;34m(self, X, y, step_score, **fit_params)\u001b[0m\n\u001b[1;32m    308\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mverbose \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m    309\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mFitting estimator with \u001b[39m\u001b[39m%d\u001b[39;00m\u001b[39m features.\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m np\u001b[39m.\u001b[39msum(support_))\n\u001b[0;32m--> 311\u001b[0m estimator\u001b[39m.\u001b[39;49mfit(X[:, features], y, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mfit_params)\n\u001b[1;32m    313\u001b[0m \u001b[39m# Get importance and rank them\u001b[39;00m\n\u001b[1;32m    314\u001b[0m importances \u001b[39m=\u001b[39m _get_feature_importances(\n\u001b[1;32m    315\u001b[0m     estimator,\n\u001b[1;32m    316\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mimportance_getter,\n\u001b[1;32m    317\u001b[0m     transform_func\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39msquare\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m    318\u001b[0m )\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/sklearn/base.py:1474\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1467\u001b[0m     estimator\u001b[39m.\u001b[39m_validate_params()\n\u001b[1;32m   1469\u001b[0m \u001b[39mwith\u001b[39;00m config_context(\n\u001b[1;32m   1470\u001b[0m     skip_parameter_validation\u001b[39m=\u001b[39m(\n\u001b[1;32m   1471\u001b[0m         prefer_skip_nested_validation \u001b[39mor\u001b[39;00m global_skip_validation\n\u001b[1;32m   1472\u001b[0m     )\n\u001b[1;32m   1473\u001b[0m ):\n\u001b[0;32m-> 1474\u001b[0m     \u001b[39mreturn\u001b[39;00m fit_method(estimator, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/sklearn/tree/_classes.py:1377\u001b[0m, in \u001b[0;36mDecisionTreeRegressor.fit\u001b[0;34m(self, X, y, sample_weight, check_input)\u001b[0m\n\u001b[1;32m   1347\u001b[0m \u001b[39m@_fit_context\u001b[39m(prefer_skip_nested_validation\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m   1348\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mfit\u001b[39m(\u001b[39mself\u001b[39m, X, y, sample_weight\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, check_input\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m):\n\u001b[1;32m   1349\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Build a decision tree regressor from the training set (X, y).\u001b[39;00m\n\u001b[1;32m   1350\u001b[0m \n\u001b[1;32m   1351\u001b[0m \u001b[39m    Parameters\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1374\u001b[0m \u001b[39m        Fitted estimator.\u001b[39;00m\n\u001b[1;32m   1375\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1377\u001b[0m     \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49m_fit(\n\u001b[1;32m   1378\u001b[0m         X,\n\u001b[1;32m   1379\u001b[0m         y,\n\u001b[1;32m   1380\u001b[0m         sample_weight\u001b[39m=\u001b[39;49msample_weight,\n\u001b[1;32m   1381\u001b[0m         check_input\u001b[39m=\u001b[39;49mcheck_input,\n\u001b[1;32m   1382\u001b[0m     )\n\u001b[1;32m   1383\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/sklearn/tree/_classes.py:472\u001b[0m, in \u001b[0;36mBaseDecisionTree._fit\u001b[0;34m(self, X, y, sample_weight, check_input, missing_values_in_feature_mask)\u001b[0m\n\u001b[1;32m    461\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    462\u001b[0m     builder \u001b[39m=\u001b[39m BestFirstTreeBuilder(\n\u001b[1;32m    463\u001b[0m         splitter,\n\u001b[1;32m    464\u001b[0m         min_samples_split,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    469\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmin_impurity_decrease,\n\u001b[1;32m    470\u001b[0m     )\n\u001b[0;32m--> 472\u001b[0m builder\u001b[39m.\u001b[39;49mbuild(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtree_, X, y, sample_weight, missing_values_in_feature_mask)\n\u001b[1;32m    474\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_outputs_ \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m \u001b[39mand\u001b[39;00m is_classifier(\u001b[39mself\u001b[39m):\n\u001b[1;32m    475\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_classes_ \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_classes_[\u001b[39m0\u001b[39m]\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "np.seterr(invalid='ignore')\n",
    "\n",
    "results_csv_path = 'data/results/all_model_results_n1500.csv'\n",
    "# Train and evaluate the models\n",
    "for target in target_columns:\n",
    "    print(f\"Training MLP for {target}...\")\n",
    "    ann_regr(df, target, n_features=1500)\n",
    "    results_df.to_csv(results_csv_path, index=False)\n",
    "\n",
    "    print(f\"Training Linear Regression for {target}...\")\n",
    "    lr_regr(df, target, n_features=1500)\n",
    "    results_df.to_csv(results_csv_path, index=False)\n",
    "    \n",
    "    # print(f\"Training Random Forest for {target}...\")\n",
    "    # rf_regr(df, target, n_features=1500)\n",
    "    \n",
    "    print(f\"Training Ridge Regression for {target}...\")\n",
    "    ridge_regr(df, target, n_features=1500)\n",
    "    results_df.to_csv(results_csv_path, index=False)\n",
    "    \n",
    "    print(f\"Training Lasso Regression for {target}...\")\n",
    "    lasso_regr(df, target, n_features=1500)\n",
    "    results_df.to_csv(results_csv_path, index=False)\n",
    "    \n",
    "    print(f\"Training Support Vector Machine for {target}...\")\n",
    "    svm_regr(df, target, n_features=1500)\n",
    "\n",
    "# results_csv_path = 'data/results/all_model_results_n1500.csv'\n",
    "results_df.to_csv(results_csv_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df = pd.read_csv(\"data/results/all_model_results_n1500.csv\")\n",
    "\n",
    "pd.set_option('display.float_format', lambda x: '%.3f' % x)\n",
    "results_df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12 (main, Sep 11 2024, 15:47:36) [GCC 11.4.0]"
  },
  "vscode": {
   "interpreter": {
    "hash": "e7370f93d1d0cde622a1f8e1c04877d8463912d04d973331ad4851f04de6915a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
